{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from io import open\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the language_words dictionary, a list of words per language\n",
    "def parseCSV(file):\n",
    "    \"\"\"returns pandas dataframe from CSV file\"\"\"\n",
    "    data = pd.read_csv(file)\n",
    "    return data\n",
    "\n",
    "def reformatData(data):\n",
    "    \"\"\"takes in pandas dataframe and outputs data in right format\"\"\"\n",
    "    language_words = defaultdict(list) \n",
    "    data = data.to_dict('split')['data']\n",
    "    \n",
    "    for entry in data:\n",
    "        language_words[entry[1]].append(entry[0])\n",
    "        \n",
    "    return dict(language_words)\n",
    "\n",
    "csv_data = parseCSV('language_dataset.csv')\n",
    "train_data, test_data = train_test_split(csv_data, test_size=0.2)\n",
    "\n",
    "training_words = reformatData(train_data)\n",
    "testing_words = reformatData(test_data)\n",
    "\n",
    "all_languages = list(training_words.keys())\n",
    "n_languages = len(all_languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse our language dataset and populate four variables we will be using later:\n",
    "\n",
    "training_words = `{'Simlish': ['Dag Dag', 'Zagadoo', ...], 'English': ['event', ..], ...}`\n",
    "\n",
    "test_words = same format at training_words\n",
    "\n",
    "all_languages = `['English', 'Spanish', 'Tolkien Elvish', ...]`\n",
    "\n",
    "n_languages = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_letters: 112\n",
      "all_letters: [' ', \"'\", '(', ')', ',', '-', '.', '/', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '²', '¹', 'Ä', 'Ö', 'Ü', 'ß', 'à', 'á', 'â', 'ä', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ñ', 'ò', 'ó', 'ô', 'ö', 'ù', 'ú', 'û', 'ü', 'þ', 'ā', 'ă', 'ē', 'ĕ', 'ī', 'ĭ', 'ō', 'ŏ', 'œ', 'ū', 'ƀ', 'ʒ', 'ˢ', '́', '̆', '̣', '̯', 'ṃ', 'ṇ', '’', '…']\n"
     ]
    }
   ],
   "source": [
    "# populate all_letters with alphabet lower and upper \n",
    "# along with spaces and a few other punctuations\n",
    "all_letters = set()\n",
    "\n",
    "for letter in string.ascii_letters + \" .,;'\" + 'áéíóúüñàèìòùçâêîôûëïäöß()-āēīōū’ā̆ē̆ī̆ō̆ăĭḗū́u̯ṇ̃þʒ¹²/':\n",
    "    all_letters.add(letter)\n",
    "\n",
    "# we also need to include special characters found in certain languages\n",
    "# primarily Tolkien Elvish and Spanish\n",
    "for lang in training_words.keys():\n",
    "    for word in training_words[lang]:\n",
    "        for letter in word:\n",
    "            all_letters.add(letter)\n",
    "            \n",
    "for lang in testing_words.keys():\n",
    "    for word in testing_words[lang]:\n",
    "        for letter in word:\n",
    "            all_letters.add(letter)      \n",
    "            \n",
    "n_letters = len(all_letters)\n",
    "all_letters = sorted(list(all_letters))\n",
    "print('n_letters:', n_letters)\n",
    "print('all_letters:', all_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn names into tensors\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    try:\n",
    "        if letter == '\\xa0':\n",
    "            letter = ' '\n",
    "        return all_letters.index(letter)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a line into a <line_length x 1 x n_letters> tensor\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    language_i = top_i[0].item()\n",
    "    return all_languages[language_i], language_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_languages) # choose a random language\n",
    "    line = randomChoice(training_words[category]) # choose a random word from that language\n",
    "    # turn the language into a 1D tensor\n",
    "    category_tensor = torch.tensor([all_languages.index(category)], dtype=torch.long)\n",
    "    # line_tensor is a 2D tensor containing a tensor for each letter\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        # Put the declaration of RNN network here\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # Put the computation for forward pass here\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        hidden = self.i2h(combined)\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, n_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iteration_CharRNN(learning_rate, category_tensor, line_tensor):\n",
    "    criterion = nn.NLLLoss()\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    # The forward process\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    # The backward process\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_charRNN(n_iters, learning_rate):\n",
    "    print_every = 1000\n",
    "    current_loss = 0\n",
    "    losses = []\n",
    "\n",
    "    def timeSince(since):\n",
    "        now = time.time()\n",
    "        s = now - since\n",
    "        m = math.floor(s / 60)\n",
    "        s -= m * 60\n",
    "        return '%dm %ds' % (m, s)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "        output, loss = train_iteration_CharRNN(learning_rate, category_tensor, line_tensor)\n",
    "        current_loss += loss\n",
    "\n",
    "        # Print iter number, loss, name and guess\n",
    "        if iter % print_every == 0:\n",
    "            guess, guess_i = categoryFromOutput(output)\n",
    "            correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "            print('%d %d%% (%s) %.4f %s / %s %s' % (\n",
    "                iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "            print('Average loss: %.4f' % (current_loss/print_every))\n",
    "            losses.append(current_loss/print_every)\n",
    "            current_loss = 0\n",
    "\n",
    "    torch.save(rnn, 'char-rnn-classification.pt')\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish the prediction function to provide predictions for any \n",
    "# input string (name) from the user\n",
    "def predict(input_line, n_predictions=7):\n",
    "#     print(\"Prediction for %s:\" % input_line)\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    # Generate the input for RNN\n",
    "    line_tensor = lineToTensor(input_line)\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    # Get the value and index of top K predictions from the output\n",
    "    # Then apply Softmax function on the scores of all category predictions so we can \n",
    "    # output the probabilities that this name belongs to different languages.\n",
    "    topv, topi = output.data.topk(n_predictions, 1, True)\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    top_prob = softmax(topv)\n",
    "    predictions = []\n",
    " \n",
    "    for i in range(3):\n",
    "        value = topv[0][i]\n",
    "        prob = top_prob[0][i] * 100\n",
    "        category_index = topi[0][i]\n",
    "#         print('%s Probability: (%.2f), Score: (%.2f)' % (all_languages[category_index], prob, value))\n",
    "        predictions.append([value, all_languages[category_index]])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 3% (0m 6s) 1.6194 au-dessus / French ✓\n",
      "Average loss: 1.8485\n",
      "2000 6% (0m 11s) 1.8997 Nerk / English ✗ (Simlish)\n",
      "Average loss: 1.6797\n",
      "3000 10% (0m 16s) 1.0605 guardar / Spanish ✓\n",
      "Average loss: 1.5347\n",
      "4000 13% (0m 21s) 1.6165 dire / Italian ✗ (French)\n",
      "Average loss: 1.4078\n",
      "5000 16% (0m 26s) 1.9698 try / Tolkien Elvish ✗ (English)\n",
      "Average loss: 1.3308\n",
      "6000 20% (0m 31s) 1.3228 perdido / Italian ✗ (Spanish)\n",
      "Average loss: 1.2848\n",
      "7000 23% (0m 36s) 0.4691 Zep tor maboo / Simlish ✓\n",
      "Average loss: 1.2020\n",
      "8000 26% (0m 41s) 0.0106 gāyā / Tolkien Elvish ✓\n",
      "Average loss: 1.1688\n",
      "9000 30% (0m 46s) 2.7895 kyelepē / German ✗ (Tolkien Elvish)\n",
      "Average loss: 1.1784\n",
      "10000 33% (0m 52s) 0.7475 essere / Italian ✓\n",
      "Average loss: 1.1065\n",
      "11000 36% (0m 57s) 1.4487 Schale / English ✗ (German)\n",
      "Average loss: 1.0730\n",
      "12000 40% (1m 2s) 1.0300 doctor / English ✓\n",
      "Average loss: 1.2171\n",
      "13000 43% (1m 7s) 0.0415 Minza bar / Simlish ✓\n",
      "Average loss: 1.1954\n",
      "14000 46% (1m 12s) 2.0874 essi / Tolkien Elvish ✗ (Italian)\n",
      "Average loss: 1.0756\n",
      "15000 50% (1m 18s) 0.6392 soldado / Spanish ✓\n",
      "Average loss: 1.0369\n",
      "16000 53% (1m 23s) 2.0077 règle / Italian ✗ (French)\n",
      "Average loss: 1.1236\n",
      "17000 56% (1m 28s) 0.8202 nunca / Spanish ✓\n",
      "Average loss: 1.0734\n",
      "18000 60% (1m 33s) 0.0315 keu̯rānā / Tolkien Elvish ✓\n",
      "Average loss: 1.0781\n",
      "19000 63% (1m 37s) 0.1701 brennen / German ✓\n",
      "Average loss: 0.9503\n",
      "20000 66% (1m 42s) 1.1894 station / Simlish ✗ (English)\n",
      "Average loss: 0.9641\n",
      "21000 70% (1m 47s) 0.6951 carta / Spanish ✓\n",
      "Average loss: 1.0291\n",
      "22000 73% (1m 51s) 0.5882 Bett / German ✓\n",
      "Average loss: 1.0297\n"
     ]
    }
   ],
   "source": [
    "losses = train_charRNN(30000, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(losses)\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Training Iteration * 1000')\n",
    "plt.ylabel('Average Loss per 1000 Iterations')\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(dataset):\n",
    "    total_words = len(dataset.index)\n",
    "    correct_predictions = 0\n",
    "    for i in dataset.index:\n",
    "        word = dataset['Word'][i]\n",
    "        language = dataset['Language'][i]\n",
    "        prediction = predict(word)[0][1]\n",
    "        if prediction == language:\n",
    "            correct_predictions += 1\n",
    "    return correct_predictions / total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training data accuracy: {:.4f}%'.format(getAccuracy(train_data) * 100))\n",
    "print('testing data accuracy: {:.4f}%'.format(getAccuracy(test_data) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
