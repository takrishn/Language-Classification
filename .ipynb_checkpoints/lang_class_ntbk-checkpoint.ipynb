{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import string\n",
    "import unicodedata\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseCSV(file):\n",
    "    data = pd.read_csv(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = parseCSV(\"language_dataset.csv\").to_dict('split')['data']\n",
    "category_lines = defaultdict(list)\n",
    "for entry in csv_data:\n",
    "    category_lines[entry[1]].append(entry[0])\n",
    "\n",
    "all_categories = list(category_lines.keys())\n",
    "\n",
    "n_categories = len(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.]])\n",
      "torch.Size([5, 1, 57])\n"
     ]
    }
   ],
   "source": [
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "#might need to use char2vec???\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "print(letterToTensor('J'))\n",
    "\n",
    "print(lineToTensor('Jones').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iteration_CharRNN(learning_rate, category_tensor, line_tensor):\n",
    "    criterion = nn.NLLLoss()\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    #The forward process\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden) #update and use the hidden layer for each line of the rnn tensor\n",
    "\n",
    "    #The backward process\n",
    "    loss = criterion(output, category_tensor) #compute the NLLLoss\n",
    "    loss.backward() #backward step\n",
    "\n",
    "    #Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data) #add parameters in-place\n",
    "\n",
    "    return output, loss.item()\n",
    "\n",
    "\n",
    "def train_charRNN(n_iters, learning_rate):\n",
    "    print_every = 1000\n",
    "\n",
    "    current_loss = 0\n",
    "\n",
    "    def timeSince(since):\n",
    "        now = time.time()\n",
    "        s = now - since\n",
    "        m = math.floor(s / 60)\n",
    "        s -= m * 60\n",
    "        return '%dm %ds' % (m, s)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "        output, loss = train_iteration_CharRNN(learning_rate, category_tensor, line_tensor)\n",
    "        current_loss += loss\n",
    "\n",
    "        # Print iter number, loss, name and guess\n",
    "        if iter % print_every == 0:\n",
    "            guess, guess_i = categoryFromOutput(output)\n",
    "            correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "            print('%d %d%% (%s) %.4f %s / %s %s' % (\n",
    "                iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "            print('Average loss: %.4f' % (current_loss/print_every))\n",
    "            current_loss = 0\n",
    "\n",
    "    torch.save(rnn, 'char-rnn-classification.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_line, n_predictions=7): #change to 8 for other???\n",
    "    print(\"Predition for %s:\" % input_line)\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    #Generate the input for RNN\n",
    "    #follow similar code to prior code for line tensor and generating output, hidden\n",
    "    line_tensor = lineToTensor(input_line) #convery input_line to tensor\n",
    "    for i in range(line_tensor.size()[0]): #go through each element of the tensor\n",
    "        output, hidden = rnn(line_tensor[i], hidden) #update and use the hidden layer for each line of the rnn tensor\n",
    "\n",
    "    #Get the value and index of top K predictions from the output\n",
    "    #Then apply Softmax function on the scores of all category predictions so we can \n",
    "    #output the probabilities that this name belongs to different languages.\n",
    "    topv, topi = output.topk(n_predictions, 1, True) #top value and top index of the top k predictions from the output\n",
    "    softmax = nn.LogSoftmax(dim=1) #softmax layer\n",
    "    top_prob = softmax(topv) #apply Softmax function on the last output value topv so result is in probability\n",
    "    predictions = []\n",
    " \n",
    "    for i in range(n_predictions):\n",
    "        value = topv[0][i].item() #topv keeps track of values\n",
    "        prob = top_prob[0][i].item() #top_prob keeps track of probability \n",
    "        category_index = topi[0][i].item() #topi keeps track of category index\n",
    "        print('%s Probability: (%.2f), Score: (%.2f)' % (all_categories[category_index], prob, value))\n",
    "        predictions.append([value, all_categories[category_index]])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 6% (0m 1s) 2.0968 décès / Spanish ✗ (French)\n",
      "Average loss: 1.1724\n",
      "2000 13% (0m 3s) 0.5247 singen / German ✓\n",
      "Average loss: 1.1695\n",
      "3000 20% (0m 5s) 3.0920 pie / French ✗ (Spanish)\n",
      "Average loss: 1.1215\n",
      "4000 26% (0m 7s) 2.4349 auri / Italian ✗ (Tolkien Elvish)\n",
      "Average loss: 1.0826\n",
      "5000 33% (0m 9s) 1.1883 columna / Italian ✗ (Spanish)\n",
      "Average loss: 1.0581\n",
      "6000 40% (0m 10s) 3.1193 two / Italian ✗ (English)\n",
      "Average loss: 1.0636\n",
      "7000 46% (0m 12s) 2.5007 Typ / Simlish ✗ (German)\n",
      "Average loss: 1.0251\n",
      "8000 53% (0m 15s) 0.7822 plaine / French ✓\n",
      "Average loss: 0.9796\n",
      "9000 60% (0m 16s) 0.6143 evento / Italian ✓\n",
      "Average loss: 1.0112\n",
      "10000 66% (0m 18s) 1.1976 comenzó / Tolkien Elvish ✗ (Spanish)\n",
      "Average loss: 1.0405\n",
      "11000 73% (0m 20s) 1.1330 Fuens / Simlish ✓\n",
      "Average loss: 1.0880\n",
      "12000 80% (0m 22s) 0.2756 posto / Italian ✓\n",
      "Average loss: 1.0587\n",
      "13000 86% (0m 24s) 2.4532 war / Simlish ✗ (English)\n",
      "Average loss: 1.0648\n",
      "14000 93% (0m 25s) 0.7530 Bild / German ✓\n",
      "Average loss: 1.0524\n",
      "15000 100% (0m 27s) 2.4584 naturaleza / Tolkien Elvish ✗ (Spanish)\n",
      "Average loss: 0.9215\n",
      "Predition for the:\n",
      "French Probability: (-1.17), Score: (-1.17)\n",
      "German Probability: (-1.29), Score: (-1.29)\n",
      "English Probability: (-1.63), Score: (-1.63)\n",
      "Simlish Probability: (-2.59), Score: (-2.59)\n",
      "Italian Probability: (-2.90), Score: (-2.90)\n",
      "Tolkien Elvish Probability: (-2.96), Score: (-2.96)\n",
      "Spanish Probability: (-3.31), Score: (-3.31)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[-1.1653292179107666, 'French'],\n",
       " [-1.2937266826629639, 'German'],\n",
       " [-1.6314804553985596, 'English'],\n",
       " [-2.5937538146972656, 'Simlish'],\n",
       " [-2.9019830226898193, 'Italian'],\n",
       " [-2.9573869705200195, 'Tolkien Elvish'],\n",
       " [-3.305363893508911, 'Spanish']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_charRNN(15000, 0.005)\n",
    "predict(\"event\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
