{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import string\n",
    "import unicodedata\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseCSV(file):\n",
    "    data = pd.read_csv(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "hi\n"
    }
   ],
   "source": [
    "csv_data = parseCSV(\"language_dataset.csv\").to_dict('split')['data']\n",
    "category_lines = defaultdict(list)\n",
    "for entry in csv_data:\n",
    "    category_lines[entry[1]].append(entry[0])\n",
    "\n",
    "all_categories = list(category_lines.keys())\n",
    "\n",
    "n_categories = len(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.]])\n",
      "torch.Size([5, 1, 57])\n"
     ]
    }
   ],
   "source": [
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "#might need to use char2vec???\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "print(letterToTensor('J'))\n",
    "\n",
    "print(lineToTensor('Jones').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iteration_CharRNN(learning_rate, category_tensor, line_tensor):\n",
    "    criterion = nn.NLLLoss()\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    #The forward process\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden) #update and use the hidden layer for each line of the rnn tensor\n",
    "\n",
    "    #The backward process\n",
    "    loss = criterion(output, category_tensor) #compute the NLLLoss\n",
    "    loss.backward() #backward step\n",
    "\n",
    "    #Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data) #add parameters in-place\n",
    "\n",
    "    return output, loss.item()\n",
    "\n",
    "\n",
    "def train_charRNN(n_iters, learning_rate):\n",
    "    print_every = 1000\n",
    "\n",
    "    current_loss = 0\n",
    "\n",
    "    def timeSince(since):\n",
    "        now = time.time()\n",
    "        s = now - since\n",
    "        m = math.floor(s / 60)\n",
    "        s -= m * 60\n",
    "        return '%dm %ds' % (m, s)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "        output, loss = train_iteration_CharRNN(learning_rate, category_tensor, line_tensor)\n",
    "        current_loss += loss\n",
    "\n",
    "        # Print iter number, loss, name and guess\n",
    "        if iter % print_every == 0:\n",
    "            guess, guess_i = categoryFromOutput(output)\n",
    "            correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "            print('%d %d%% (%s) %.4f %s / %s %s' % (\n",
    "                iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "            print('Average loss: %.4f' % (current_loss/print_every))\n",
    "            current_loss = 0\n",
    "\n",
    "    torch.save(rnn, 'char-rnn-classification.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_line, n_predictions=7): #change to 8 for other???\n",
    "    print(\"Predition for %s:\" % input_line)\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    #Generate the input for RNN\n",
    "    #follow similar code to prior code for line tensor and generating output, hidden\n",
    "    line_tensor = lineToTensor(input_line) #convery input_line to tensor\n",
    "    for i in range(line_tensor.size()[0]): #go through each element of the tensor\n",
    "        output, hidden = rnn(line_tensor[i], hidden) #update and use the hidden layer for each line of the rnn tensor\n",
    "\n",
    "    #Get the value and index of top K predictions from the output\n",
    "    #Then apply Softmax function on the scores of all category predictions so we can \n",
    "    #output the probabilities that this name belongs to different languages.\n",
    "    topv, topi = output.topk(n_predictions, 1, True) #top value and top index of the top k predictions from the output\n",
    "    softmax = nn.LogSoftmax(dim=1) #softmax layer\n",
    "    top_prob = softmax(topv) #apply Softmax function on the last output value topv so result is in probability\n",
    "    predictions = []\n",
    " \n",
    "    for i in range(n_predictions):\n",
    "        value = topv[0][i].item() #topv keeps track of values\n",
    "        prob = top_prob[0][i].item() #top_prob keeps track of probability \n",
    "        category_index = topi[0][i].item() #topi keeps track of category index\n",
    "        print('%s Probability: (%.2f), Score: (%.2f)' % (all_categories[category_index], prob, value))\n",
    "        predictions.append([value, all_categories[category_index]])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 6% (0m 1s) 1.2088 Lass / German ✗ (Simlish)\n",
      "Average loss: 0.9619\n",
      "2000 13% (0m 3s) 1.0212 caccia / Spanish ✗ (Italian)\n",
      "Average loss: 0.9289\n",
      "3000 20% (0m 5s) 0.0407 Sugnorg / Simlish ✓\n",
      "Average loss: 0.9911\n",
      "4000 26% (0m 7s) 0.2714 vetro / Italian ✓\n",
      "Average loss: 0.8501\n",
      "5000 33% (0m 9s) 0.0282 wō / Tolkien Elvish ✓\n",
      "Average loss: 0.8927\n",
      "6000 40% (0m 10s) 0.3136 joy / English ✓\n",
      "Average loss: 0.8589\n",
      "7000 46% (0m 12s) 0.0097 kirtē / Tolkien Elvish ✓\n",
      "Average loss: 0.9140\n",
      "8000 53% (0m 14s) 0.5481 énergie / French ✓\n",
      "Average loss: 0.8555\n",
      "9000 60% (0m 16s) 0.5057 vestido / Spanish ✓\n",
      "Average loss: 0.8393\n",
      "10000 66% (0m 18s) 1.3809 besoin / English ✗ (French)\n",
      "Average loss: 0.9118\n",
      "11000 73% (0m 19s) 0.5760 queue / French ✓\n",
      "Average loss: 0.9134\n",
      "12000 80% (0m 21s) 0.0007 wegō(n) / Tolkien Elvish ✓\n",
      "Average loss: 0.9036\n",
      "13000 86% (0m 23s) 2.0227 Firma / Simlish ✗ (German)\n",
      "Average loss: 0.8203\n",
      "14000 93% (0m 25s) 0.2864 chick / English ✓\n",
      "Average loss: 0.8867\n",
      "15000 100% (0m 27s) 0.2091 sapnā / Tolkien Elvish ✓\n",
      "Average loss: 0.8665\n",
      "Predition for event:\n",
      "French Probability: (-0.50), Score: (-0.50)\n",
      "English Probability: (-1.32), Score: (-1.32)\n",
      "Italian Probability: (-2.68), Score: (-2.68)\n",
      "Spanish Probability: (-3.37), Score: (-3.37)\n",
      "German Probability: (-4.23), Score: (-4.23)\n",
      "Tolkien Elvish Probability: (-4.81), Score: (-4.81)\n",
      "Simlish Probability: (-9.08), Score: (-9.08)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[-0.5009896159172058, 'French'],\n",
       " [-1.315126895904541, 'English'],\n",
       " [-2.6837406158447266, 'Italian'],\n",
       " [-3.3664541244506836, 'Spanish'],\n",
       " [-4.228370666503906, 'German'],\n",
       " [-4.813390254974365, 'Tolkien Elvish'],\n",
       " [-9.082365036010742, 'Simlish']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_charRNN(15000, 0.005)\n",
    "predict(\"event\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}